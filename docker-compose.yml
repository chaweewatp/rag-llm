version: "3.8"

services:
  # ingestion:
  #   build: ./ingestion
  #   container_name: ingestion_service
  #   volumes:
  #     - ./ingestion:/app
  #   working_dir: /app
  #   command: ["python", "pea_reg.py"]
  #   networks:
  #     - my_network

  ollama:
    image: ollama/ollama # Official Ollama image
    container_name: ollama_service
    ports:
      - "11435:11434"
    networks:
      - my_network
    # command: ["ollama_service", "ollama", "run", "llama3.2:1b"]

    # entrypoint:
    #   ["/bin/sh", "-c", "echo 'Starting Ollama...';  ollama run llama3.2:1b"]
  ## docker exec -it ollama_service ollama run llama3.2:1b
  be:
    build: ./be
    container_name: flask_backend
    ports:
      - "8001:8001"
    volumes:
      - ./be:/app
    working_dir: /app
    command: ["flask", "run", "--host=0.0.0.0", "--port=8001"]
    environment:
      - FLASK_APP=app.py
      - FLASK_ENV=development
    networks:
      - my_network
    # depends_on:
    #   - ollama # Ensure Ollama starts first

  fe:
    build: ./fe
    container_name: streamlit_frontend
    ports:
      - "8501:8501"
    volumes:
      - ./fe:/app
    working_dir: /app
    command:
      [
        "streamlit",
        "run",
        "app.py",
        "--server.port=8501",
        "--server.address=0.0.0.0",
      ]
    networks:
      - my_network

networks:
  my_network:
    driver: bridge

volumes:
  ollama_data:
